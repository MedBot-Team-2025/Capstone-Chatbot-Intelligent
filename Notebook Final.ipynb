{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'int' from 'numpy' (C:\\Users\\antop\\anaconda3\\envs\\learn-env\\lib\\site-packages\\numpy\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f7de38eaed92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\class_weight.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_deprecate_positional_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcontextlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msuppress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_get_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNonBLASDotWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPositiveSpectrumWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlsqr\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msparse_lsqr\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMaskedArray\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_MaskedArray\u001b[0m  \u001b[1;31m# TODO: remove in 0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\stats\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m \"\"\"\n\u001b[1;32m--> 388\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmorestats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\stats\\stats.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcdist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmeasurements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m from scipy._lib._util import (_lazywhere, check_random_state, MapWrapper,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\spatial\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_plotutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_procrustes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprocrustes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_geometric_slerp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgeometric_slerp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[0m__all__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\spatial\\_geometric_slerp.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0meuclidean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\spatial\\distance.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_hausdorff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrel_entr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\special\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_ufuncs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_basic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    637\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_basic\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\special\\_basic.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m                       poch, binom, hyp0f1)\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspecfun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0morthogonal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_comb\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_comb_int\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\scipy\\special\\orthogonal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;31m# SciPy imports.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n\u001b[0m\u001b[0;32m     80\u001b[0m                    hstack, arccos, arange)\n\u001b[0;32m     81\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'int' from 'numpy' (C:\\Users\\antop\\anaconda3\\envs\\learn-env\\lib\\site-packages\\numpy\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Importation des librairies\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('stopwords')\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import threading\n",
    "from tkinter import *\n",
    "from tkinter import scrolledtext\n",
    "\n",
    "print('Toutes les librairies ont été importées avec succès !')\n",
    "print('='*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Charger l'ensemble de données\n",
    "with open('data/healthcare.json','r',encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "intents = {'intents':[]}\n",
    "#\n",
    "for convo in data:\n",
    "    tag = convo.get(\"agent_selected_tool\", \"general\").replace(\" \",\"_\").lower()\n",
    "\n",
    "\n",
    "    pattern = convo.get(\"user_1\", \"\")\n",
    "    response = convo.get(\"agent_initial_response\", \"\")\n",
    "\n",
    "\n",
    "    if pattern and response:\n",
    "        intents['intents'].append({\n",
    "            \"tag\":tag,\n",
    "            \"patterns\":[pattern],\n",
    "            \"responses\":[response]\n",
    "        })\n",
    "\n",
    "# Save the new dataset\n",
    "with open(\"data/healthcare_intents.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(intents, f, indent=4)\n",
    "\n",
    "\n",
    "print(\"Les intentions de l'ensemble de données ont été créées avec succès !\")\"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe principale du Modele\n",
    "class ChatbotModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.drop = nn.Dropout(0.4)\n",
    "        self.out = nn.Linear(64, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.drop(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.drop(x)\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.drop(x)\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.out(x)  \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison entre différents modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assistant chatbot\n",
    "class ChatbotAssistant:\n",
    "    def __init__(self, intents_path, function_mappings=None, device=None):\n",
    "        self.intents_path = intents_path\n",
    "        self.function_mappings = function_mappings\n",
    "\n",
    "        self.documents = []\n",
    "        self.intents = []\n",
    "        self.intents_responses = {}\n",
    "\n",
    "        self.vectorizer = None\n",
    "        self.label_names = None\n",
    "        self.model = None\n",
    "\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        for r in [\"punkt\", \"wordnet\", \"omw-1.4\", \"stopwords\"]:\n",
    "            try:\n",
    "                nltk.data.find(f\"corpora/{r}\")\n",
    "            except LookupError:\n",
    "                nltk.download(r)\n",
    "\n",
    "    \n",
    "    # Prétraitement du texte\n",
    "    @staticmethod\n",
    "    def preprocess_text(text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "        lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "        return \" \".join(lemmas)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Analyser les intentions\n",
    "    def parse_intents(self):\n",
    "        if not os.path.exists(self.intents_path):\n",
    "            raise FileNotFoundError(f\"File not found: {self.intents_path}\")\n",
    "\n",
    "        with open(self.intents_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.documents = []\n",
    "        self.intents = []\n",
    "        self.intents_responses = {}\n",
    "\n",
    "        for intent in data.get(\"intents\", []):\n",
    "            tag = intent.get(\"tag\")\n",
    "            if tag is None:\n",
    "                continue\n",
    "            if tag not in self.intents:\n",
    "                self.intents.append(tag)\n",
    "                self.intents_responses[tag] = intent.get(\"responses\", [])\n",
    "\n",
    "            for pattern in intent.get(\"patterns\", []):\n",
    "                cleaned = self.preprocess_text(pattern)\n",
    "                self.documents.append((cleaned, tag))\n",
    "\n",
    "        print(f\"Loaded {len(self.documents)} patterns across {len(self.intents)} intents.\")\n",
    "\n",
    "    \n",
    "    \n",
    "    # Créer des caractéristiques TF-IDF\n",
    "    def build_features(self, max_features=2000):\n",
    "        texts = [doc[0] for doc in self.documents]\n",
    "        tags = [doc[1] for doc in self.documents]\n",
    "\n",
    "        self.label_names = sorted(list(set(self.intents)))\n",
    "        tag_to_idx = {t: i for i, t in enumerate(self.label_names)}\n",
    "        self.y = np.array([tag_to_idx[t] for t in tags])\n",
    "\n",
    "        self.vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "        self.X = self.vectorizer.fit_transform(texts).toarray()\n",
    "        print(f\"TF-IDF built: X.shape={self.X.shape}, y.shape={self.y.shape}\")\n",
    "\n",
    "        \n",
    "     \n",
    "    # Entraîner et comparer des modèles\n",
    "    def compare_models(self, test_size=0.2, random_state=42, max_features=2000):\n",
    "        \"\"\"Compare plusieurs modèles ML sur le même dataset (TF-IDF).\"\"\"\n",
    "    \n",
    "\n",
    "        try:\n",
    "            from xgboost import XGBClassifier\n",
    "            xgb_available = True\n",
    "        except ImportError:\n",
    "            print(\" XGBoost not installed, skipping it.\")\n",
    "            xgb_available = False\n",
    "\n",
    "        # Préparation des données\n",
    "        if self.X is None or self.y is None:\n",
    "            self.build_features(max_features=max_features)\n",
    "        # Filtrer les classes rares (<2 exemples)\n",
    "        cnt = Counter(self.y)\n",
    "        valid = [cls for cls, c in cnt.items() if c >= 2]\n",
    "\n",
    "        if len(valid) < 2:\n",
    "            raise ValueError(\"Il n'y a pas assez de classes valides avec au moins 2 échantillons chacune !\")\n",
    "\n",
    "        mask = np.isin(self.y, valid)\n",
    "        self.X = self.X[mask]\n",
    "        self.y = self.y[mask]\n",
    "\n",
    "        print(f\"Filtered dataset: {len(self.y)} samples, {len(valid)} valid classes\")\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.X, self.y, test_size=test_size, random_state=random_state,\n",
    "            stratify=self.y if len(set(self.y)) > 1 else None\n",
    "        )\n",
    "\n",
    "        models = {\n",
    "            \"Logistic Regression\": LogisticRegression(max_iter=1000, n_jobs=-1),\n",
    "            \"SVM\": LinearSVC(),\n",
    "            \"Naive Bayes\": MultinomialNB(),\n",
    "            \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "            \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "            \"MLP (Sklearn)\": MLPClassifier(hidden_layer_sizes=(512, 256, 128),\n",
    "                                        activation='relu', max_iter=300, random_state=42)\n",
    "        }\n",
    "\n",
    "        if xgb_available:\n",
    "            models[\"XGBoost\"] = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "        results = {}\n",
    "        print(\"\\n Comparaison des modèles...\\n\")\n",
    "\n",
    "        for name, model in models.items():\n",
    "            print(f\"\\n Training {name} ...\")\n",
    "            start = time.time()\n",
    "            model.fit(X_train, y_train)\n",
    "            preds = model.predict(X_test)\n",
    "            acc = accuracy_score(y_test, preds)\n",
    "            elapsed = time.time() - start\n",
    "            results[name] = acc\n",
    "            print(f\" Accuracy: {acc:.4f}  |  Time: {elapsed:.2f}s\")\n",
    "            print(classification_report(y_test, preds, zero_division=0))\n",
    "\n",
    "\n",
    "        \n",
    "        # --- Comparaison avec Neural Network (Torch)\n",
    "        print(\"\\n Neural Network (Torch)\")\n",
    "        input_size = self.X.shape[1]\n",
    "        output_size = len(self.label_names)\n",
    "        model = ChatbotModel(input_size, output_size)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "        X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "        X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
    "        train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=32, shuffle=True)\n",
    "\n",
    "        for epoch in range(50):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for Xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = model(Xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            print(f\"Epoch {epoch+1}/50 | Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds_nn = torch.argmax(model(X_test_t), dim=1).numpy()\n",
    "\n",
    "        acc_nn = accuracy_score(y_test, preds_nn)\n",
    "        results[\"Neural Network\"] = acc_nn\n",
    "        print(\"\\n Précision des réseaux neuronaux:\", acc_nn)\n",
    "        unique_labels = np.unique(y_test)\n",
    "        filtered_names = [self.label_names[i] for i in unique_labels if i < len(self.label_names)]\n",
    "        print(classification_report(y_test, preds_nn, labels=unique_labels, target_names=filtered_names))\n",
    "\n",
    "        # Résumé global\n",
    "        print(\"\\n Résumé de la comparaison des modèles:\")\n",
    "        for k, v in results.items():\n",
    "            print(f\"{k:<25} → {v:.4f}\")\n",
    "\n",
    "        best_model = max(results, key=results.get)\n",
    "        print(f\"\\n Best model: {best_model} with accuracy = {results[best_model]:.4f}\")\n",
    "\n",
    "        # Graphique comparatif\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(list(results.keys()), list(results.values()), color='skyblue')\n",
    "        plt.xlabel(\"Accuracy\")\n",
    "        plt.title(\"Comparaison des modèles\")\n",
    "        plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exécution\n",
    "assistant = ChatbotAssistant(\"data/healthcare_intents.json\")\n",
    "assistant.parse_intents()\n",
    "assistant.build_features(max_features=2000)\n",
    "\n",
    "results = assistant.compare_models()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choix du meilleur modèle\n",
    "    Après avoir comparé plusieurs modèles sur la base de leur taux de précision (Accuracy), nous avons décidé d’entraîner nos données à l’aide de deux modèles : un réseau de neurones (Neural Network) et une machine à vecteurs de support (SVM).\n",
    "    Le SVM a été retenu car il a obtenu la meilleure précision globale parmi les modèles testés.\n",
    "    Le Neural Network, quant à lui, a été conservé puisque ses performances étaient proches de celles du SVM, et qu’il représente le modèle initialement prévu dans la conception de notre chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe principale du Chatbot\n",
    "    \n",
    "class ChatbotAssistant:\n",
    "    def __init__(self, intents_path, function_mappings=None, device=None):\n",
    "        self.intents_path = intents_path\n",
    "        self.function_mappings = function_mappings\n",
    "\n",
    "        self.documents = []              \n",
    "        self.intents = []                \n",
    "        self.intents_responses = {}      \n",
    "\n",
    "        \n",
    "        self.vectorizer = None\n",
    "        self.label_names = None\n",
    "        self.model = None\n",
    "\n",
    "        \n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "        \n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    \n",
    "    # Prétraitement du texte\n",
    "    @staticmethod\n",
    "    def preprocess_text(text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "        lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "        return \" \".join(lemmas)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Chargement du fichier intents.json\n",
    "    def parse_intents(self):\n",
    "        if not os.path.exists(self.intents_path):\n",
    "            raise FileNotFoundError(f\"File not found: {self.intents_path}\")\n",
    "\n",
    "        with open(self.intents_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.documents = []\n",
    "        self.intents = []\n",
    "        self.intents_responses = {}\n",
    "\n",
    "        for intent in data.get(\"intents\", []):\n",
    "            tag = intent.get(\"tag\")\n",
    "            if tag is None:\n",
    "                continue\n",
    "            if tag not in self.intents:\n",
    "                self.intents.append(tag)\n",
    "                self.intents_responses[tag] = intent.get(\"responses\", [])\n",
    "\n",
    "            for pattern in intent.get(\"patterns\", []):\n",
    "                cleaned = self.preprocess_text(pattern)\n",
    "               \n",
    "                self.documents.append((cleaned, tag))\n",
    "\n",
    "        print(f\"Loaded {len(self.documents)} patterns across {len(self.intents)} intents.\")\n",
    "\n",
    "    \n",
    "    \n",
    "    #  Vectorisation TF-IDF\n",
    "    def build_features(self, max_features=2000):\n",
    "        texts = [doc[0] for doc in self.documents]\n",
    "        tags = [doc[1] for doc in self.documents]\n",
    "\n",
    "        \n",
    "        self.label_names = sorted(list(set(self.intents)))\n",
    "        tag_to_idx = {t: i for i, t in enumerate(self.label_names)}\n",
    "        self.y = np.array([tag_to_idx[t] for t in tags])\n",
    "\n",
    "        \n",
    "        self.vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "        X = self.vectorizer.fit_transform(texts).toarray()\n",
    "        self.X = X\n",
    "        print(f\"TF-IDF built: X.shape = {self.X.shape}, y.shape = {self.y.shape}\")\n",
    "\n",
    "        cnt = Counter(self.y)\n",
    "        valid = [cls for cls, c in cnt.items() if c >= 2]\n",
    "        mask = np.isin(self.y, valid)\n",
    "        self.X = self.X[mask]\n",
    "        self.y = self.y[mask]\n",
    "        print(\"Filtered shapes:\", self.X.shape, self.y.shape)\n",
    "\n",
    "        old_label_names = self.label_names\n",
    "        new_label_names = [old_label_names[i] for i in valid]\n",
    "\n",
    "        old_to_new = {old_idx: new_idx for new_idx, old_idx in enumerate(valid)}\n",
    "\n",
    "        self.y = np.array([old_to_new[int(old)] for old in self.y if int(old) in old_to_new])\n",
    "\n",
    "        self.label_names = new_label_names\n",
    "        \n",
    "        print(f\"Nouvelle taille de label_names : {len(self.label_names)}\")\n",
    "   \n",
    "    \n",
    "    #  Entraînement du modèle\n",
    "    def train_model(self,\n",
    "                    batch_size=32,\n",
    "                    lr=1e-3,\n",
    "                    epochs=50,\n",
    "                    max_features=2000,\n",
    "                    test_size=0.2,\n",
    "                    scheduler_step=None,\n",
    "                    scheduler_gamma=0.8,\n",
    "                    random_state=42):\n",
    "        \n",
    "        if self.X is None or self.y is None:\n",
    "            print(\"Building features (TF-IDF)...\")\n",
    "            self.build_features(max_features=max_features)\n",
    "\n",
    "    # Filtrer les classes rares (<2 exemples)\n",
    "        cnt = Counter(self.y)\n",
    "        valid = [cls for cls, c in cnt.items() if c >= 2]\n",
    "\n",
    "        if len(valid) < 2:\n",
    "            raise ValueError(\"Il n'y a pas assez de classes valides avec au moins 2 échantillons chacune !\")\n",
    "\n",
    "        mask = np.isin(self.y, valid)\n",
    "        self.X = self.X[mask]\n",
    "        self.y = self.y[mask]\n",
    "\n",
    "        print(f\"Filtered dataset: {len(self.y)} samples, {len(valid)} valid classes\")\n",
    "\n",
    "        #   Split train/test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.X, self.y, test_size=test_size, random_state=random_state,\n",
    "            stratify=self.y if len(set(self.y)) > 1 else None\n",
    "    )\n",
    "        print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "\n",
    "        # Données Tensor\n",
    "        X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "        X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "        train_loader = DataLoader(TensorDataset(X_train_t, y_train_t),\n",
    "                              batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Création du modèle\n",
    "        input_size = self.X.shape[1]\n",
    "        output_size = len(valid)  \n",
    "\n",
    "        self.model = ChatbotModel(input_size, output_size)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step, gamma=scheduler_gamma)\n",
    "\n",
    "        # Entraînement\n",
    "        print(\"\\n Starting training...\\n\")\n",
    "        loss_values = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for batch_X, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            loss_values.append(epoch_loss)\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        # Loss Graphe\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(loss_values, label=\"Training Loss\", color=\"blue\")\n",
    "        plt.title(\"Training Loss Curve\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # Evaluation\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = torch.argmax(self.model(X_test_t), dim=1).numpy()\n",
    "\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        print(\"\\n=== Test set evaluation ===\")\n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    #   Mettre à jour les noms d’intents valides\n",
    "        unique_labels = np.unique(y_test)\n",
    "        filtered_names = [self.label_names[i] for i in unique_labels if i < len(self.label_names)]\n",
    "\n",
    "        print(classification_report(\n",
    "        y_test, preds,\n",
    "        labels=unique_labels,\n",
    "        target_names=filtered_names,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "        print(\"\\n Model training + evaluation complete.\")\n",
    "\n",
    "         \n",
    "        print(\"Model training + evaluation complete.\")\n",
    "        unique_labels = np.unique(y_test)\n",
    "        filtered_names = [self.label_names[i] for i in unique_labels]\n",
    "    \n",
    "    \n",
    "    # Sauvegarde / Chargement\n",
    "    def save_all(self, model_path=\"chat_model.pth\", meta_path=\"meta.pkl\"):\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "        meta = {\n",
    "            \"label_names\": self.label_names,\n",
    "            \"vectorizer\": self.vectorizer  \n",
    "        }\n",
    "        \n",
    "        with open(meta_path, \"wb\") as f:\n",
    "            pickle.dump(meta, f)\n",
    "        print(f\"Saved model to {model_path} and meta to {meta_path}.\")\n",
    "\n",
    "    def load_all(self, model_path=\"chat_model_tfidf.pth\", meta_path=\"meta_tfidf.pkl\"):\n",
    "        with open(meta_path, \"rb\") as f:\n",
    "            meta = pickle.load(f)\n",
    "        self.label_names = meta[\"label_names\"]\n",
    "        self.vectorizer = meta[\"vectorizer\"]\n",
    "        input_size = self.vectorizer.max_features if hasattr(self.vectorizer, \"max_features\") and self.vectorizer.max_features is not None else self.vectorizer.vocabulary_.__len__()\n",
    "        \n",
    "        input_size = len(self.vectorizer.get_feature_names_out())\n",
    "        output_size = len(self.label_names)\n",
    "        self.model = ChatbotModel(input_size, output_size).to(self.device)\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.eval()\n",
    "        print(f\"Loaded model and meta from {model_path}, {meta_path}.\")\n",
    "\n",
    "    \n",
    "    \n",
    "    # Interaction avec l'utilisateur\n",
    "    def process_message(self, input_message, threshold=0.65, log_uncertain=True):\n",
    "        if self.model is None or self.vectorizer is None:\n",
    "            raise RuntimeError(\"Modèle ou vectoriseur non chargé. Appelez d'abord train_model() ou load_all().\")\n",
    "\n",
    "        cleaned = self.preprocess_text(input_message)\n",
    "        vec = self.vectorizer.transform([cleaned]).toarray()\n",
    "        input_t = torch.tensor(vec, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_t)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            confidence, idx = torch.max(probs, dim=1)\n",
    "            confidence = confidence.item()\n",
    "            pred_idx = idx.item()\n",
    "            predicted_intent = self.label_names[pred_idx]\n",
    "\n",
    "        if confidence >= threshold:\n",
    "            \n",
    "            resp = random.choice(self.intents_responses.get(predicted_intent, [\"Sorry, I don't have a response.\"]))\n",
    "            return f\"({confidence*100:.1f}% confident) {resp}\"\n",
    "        else:\n",
    "            if log_uncertain:\n",
    "                with open(\"uncertain_inputs.log\", \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(input_message.strip() + \"\\n\")\n",
    "            return \"I'm not sure I understood that. Could you rephrase?\"\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatbotAssistant' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c4030f7802b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0massistant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mChatbotAssistant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/healthcare_intents.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0massistant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_intents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ChatbotAssistant' is not defined"
     ]
    }
   ],
   "source": [
    "# Exécution\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    assistant = ChatbotAssistant(\"data/healthcare_intents.json\")\n",
    "    assistant.parse_intents()\n",
    "    \n",
    "    assistant.train_model(batch_size=32, lr=1e-3, epochs=50, max_features=2000, test_size=0.2, scheduler_step=15, scheduler_gamma=0.8)\n",
    "    #assistant.save_all(\"chat_model_tfidf.pth\", \"meta_tfidf.pkl\")\n",
    "\n",
    "    assistant.load_all('chat_model_tfidf.pth','meta_tfidf.pkl')\n",
    "\n",
    "     \n",
    "    print(\"\\nYou can chat. Type /quit to exit.\")\n",
    "    while True:\n",
    "        msg = input(\"You: \")\n",
    "        if msg.strip().lower() == \"/quit\":\n",
    "            break\n",
    "        print(\"Chatbot:\", assistant.process_message(msg, threshold=0.65))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classe principale du Chatbot\n",
    "\n",
    "class ChatbotAssistant:\n",
    "    def __init__(self, intents_path, function_mappings=None):\n",
    "        self.intents_path = intents_path\n",
    "        self.function_mappings = function_mappings\n",
    "\n",
    "        self.documents = []              \n",
    "        self.intents = []                \n",
    "        self.intents_responses = {}      \n",
    "\n",
    "        self.vectorizer = None\n",
    "        self.label_names = None\n",
    "        self.model = None\n",
    "\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "\n",
    "    # Prétraitement du texte\n",
    "    @staticmethod\n",
    "    def preprocess_text(text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "        lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "        return \" \".join(lemmas)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Lecture du fichier intents.json\n",
    "    def parse_intents(self):\n",
    "        if not os.path.exists(self.intents_path):\n",
    "            raise FileNotFoundError(f\"File not found: {self.intents_path}\")\n",
    "\n",
    "        with open(self.intents_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.documents = []\n",
    "        self.intents = []\n",
    "        self.intents_responses = {}\n",
    "\n",
    "        for intent in data.get(\"intents\", []):\n",
    "            tag = intent.get(\"tag\")\n",
    "            if tag is None:\n",
    "                continue\n",
    "            if tag not in self.intents:\n",
    "                self.intents.append(tag)\n",
    "                self.intents_responses[tag] = intent.get(\"responses\", [])\n",
    "\n",
    "            for pattern in intent.get(\"patterns\", []):\n",
    "                cleaned = self.preprocess_text(pattern)\n",
    "                self.documents.append((cleaned, tag))\n",
    "\n",
    "        print(f\" Loaded {len(self.documents)} patterns across {len(self.intents)} intents.\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Vectorisation TF-IDF\n",
    "    def build_features(self, max_features=2000):\n",
    "        texts = [doc[0] for doc in self.documents]\n",
    "        tags = [doc[1] for doc in self.documents]\n",
    "\n",
    "        self.label_names = sorted(list(set(self.intents)))\n",
    "        tag_to_idx = {t: i for i, t in enumerate(self.label_names)}\n",
    "        self.y = np.array([tag_to_idx[t] for t in tags])\n",
    "\n",
    "        self.vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "        X = self.vectorizer.fit_transform(texts).toarray()\n",
    "        self.X = X\n",
    "        print(f\" TF-IDF built: X.shape = {self.X.shape}, y.shape = {self.y.shape}\")\n",
    "\n",
    "        # Filtrer les classes trop rares\n",
    "        cnt = Counter(self.y)\n",
    "        valid = [cls for cls, c in cnt.items() if c >= 2]\n",
    "        mask = np.isin(self.y, valid)\n",
    "        self.X = self.X[mask]\n",
    "        self.y = self.y[mask]\n",
    "\n",
    "        old_label_names = self.label_names\n",
    "        new_label_names = [old_label_names[i] for i in valid]\n",
    "        old_to_new = {old_idx: new_idx for new_idx, old_idx in enumerate(valid)}\n",
    "        self.y = np.array([old_to_new[int(old)] for old in self.y if int(old) in old_to_new])\n",
    "        self.label_names = new_label_names\n",
    "\n",
    "        print(f\" Filtered dataset: {len(self.y)} samples, {len(self.label_names)} valid classes.\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Entraînement du modèle SVM\n",
    "    def train_model(self, test_size=0.2, random_state=42):\n",
    "        if self.X is None or self.y is None:\n",
    "            self.build_features()\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.X, self.y, test_size=test_size, random_state=random_state, stratify=self.y\n",
    "        )\n",
    "\n",
    "        print(\"\\n Training Support Vector Machine...\")\n",
    "        self.model = SVC(kernel=\"rbf\", probability=True, class_weight=\"balanced\", C=2, gamma='scale')\n",
    "\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "        preds = self.model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        print(f\" SVM Accuracy: {acc:.4f}\")\n",
    "\n",
    "        unique_labels = np.unique(y_test)\n",
    "        filtered_names = [self.label_names[i] for i in unique_labels if i < len(self.label_names)]\n",
    "\n",
    "        print(\"\\n Classification Report:\")\n",
    "        print(classification_report(y_test, preds, labels=unique_labels, target_names=filtered_names, zero_division=0))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Sauvegarde / Chargement avec joblib\n",
    "    def save_all(self, model_path=\"svm_model.pkl\", meta_path=\"svm_meta.pkl\"):\n",
    "        joblib.dump(self.model, model_path)\n",
    "        joblib.dump({\n",
    "            \"vectorizer\": self.vectorizer,\n",
    "            \"label_names\": self.label_names,\n",
    "            \"intents_responses\": self.intents_responses\n",
    "        }, meta_path)\n",
    "        print(f\" Saved model to {model_path} and meta to {meta_path}.\")\n",
    "\n",
    "\n",
    "    def load_all(self, model_path=\"svm_model.pkl\", meta_path=\"svm_meta.pkl\"):\n",
    "        self.model = joblib.load(model_path)\n",
    "        meta = joblib.load(meta_path)\n",
    "        self.vectorizer = meta[\"vectorizer\"]\n",
    "        self.label_names = meta[\"label_names\"]\n",
    "        self.intents_responses = meta[\"intents_responses\"]\n",
    "        print(f\" Loaded model and meta from {model_path}, {meta_path}.\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Prédiction d’un message utilisateur\n",
    "    def process_message(self, input_message, threshold=0.05, log_uncertain=True):\n",
    "        if self.model is None or self.vectorizer is None:\n",
    "            raise RuntimeError(\"Modèle ou vectoriseur non chargé. Appelez d'abord train_model() ou load_all().\")\n",
    "\n",
    "        cleaned = self.preprocess_text(input_message)\n",
    "        vec = self.vectorizer.transform([cleaned]).toarray()\n",
    "\n",
    "        probs = self.model.predict_proba(vec)[0]\n",
    "        pred_idx = np.argmax(probs)\n",
    "        confidence = probs[pred_idx]\n",
    "        predicted_intent = self.label_names[pred_idx]\n",
    "\n",
    "        if confidence >= threshold:\n",
    "            resp = random.choice(self.intents_responses.get(predicted_intent, [\"Sorry, I don't have a response.\"]))\n",
    "            return f\"({confidence*100:.1f}% confident) {resp}\"\n",
    "        else:\n",
    "            if log_uncertain:\n",
    "                with open(\"uncertain_inputs.log\", \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(input_message.strip() + \"\\n\")\n",
    "            return \"I'm not sure I understood that. Could you rephrase?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File not found: data/healthcare_intents.json",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6c1748c5a442>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0massistant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mChatbotAssistant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/healthcare_intents.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0massistant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_intents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0massistant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0massistant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-c1fc9c99c62e>\u001b[0m in \u001b[0;36mparse_intents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparse_intents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintents_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"File not found: {self.intents_path}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintents_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File not found: data/healthcare_intents.json"
     ]
    }
   ],
   "source": [
    "# Exécution\n",
    "\n",
    "assistant = ChatbotAssistant(\"data/healthcare_intents.json\")\n",
    "assistant.parse_intents()\n",
    "assistant.build_features()\n",
    "assistant.train_model()\n",
    "\n",
    "#assistant.save_all(\"svm_model.pkl\", \"svm_meta.pkl\")\n",
    "\n",
    "# --- Later ---\n",
    "assistant.load_all(\"svm_model.pkl\", \"svm_meta.pkl\")\n",
    "print(\"\\nYou can chat. Type /quit to exit.\")\n",
    "while True:\n",
    "    msg = input(\"You: \")\n",
    "    if msg.strip().lower() == \"/quit\":\n",
    "        break\n",
    "    print(\"Chatbot:\", assistant.process_message(msg, threshold=0.05))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Déploiement et Expérience Utilisateur\n",
    "Après avoir développé et comparé les deux modèles, nous avons retenu le réseau de neurones (Neural Network) pour le déploiement.\n",
    "Bien que la performance du SVM soit comparable à celle du Neural Network, ce dernier présente un seuil de confiance plus élevé et plus fiable (65 %) contre seulement 0,05 % pour le SVM.\n",
    "Un seuil aussi faible rend le SVM plus susceptible de produire des hallucinations ou des réponses incorrectes.\n",
    "Par souci de fiabilité et de robustesse, nous avons donc privilégié le Neural Network pour la mise en production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# COULEURS ET STYLE\n",
    "\n",
    "BG_COLOR = \"#1E1E1E\"\n",
    "BOT_COLOR = \"#2D2D2D\"\n",
    "USER_COLOR = \"#0078D7\"\n",
    "TEXT_COLOR = \"#EAECEE\"\n",
    "INPUT_COLOR = \"#2C2C2C\"\n",
    "\n",
    "FONT = \"Helvetica 12\"\n",
    "FONT_BOLD = \"Helvetica 12 bold\"\n",
    "\n",
    "BOT_NAME = \"MedBot 🤖\"\n",
    "\n",
    "# Application de chat (intégration GUI + modèle)\n",
    "from __main__ import ChatbotAssistant\n",
    "class ModernChatApp:\n",
    "    def __init__(self):\n",
    "        self.window = Tk()\n",
    "        self.window.title(\"MedBot - Healthcare Chat\")\n",
    "        self.window.configure(bg=BG_COLOR)\n",
    "        self.window.geometry(\"500x600\")\n",
    "        self.window.resizable(False, False)\n",
    "\n",
    "        self.setup_ui()\n",
    "\n",
    "        # Initialiser l'assistant chatbot\n",
    "        self.assistant = ChatbotAssistant(\"data/healthcare_intents.json\")\n",
    "        self.assistant.parse_intents()\n",
    "\n",
    "        # Essayer de charger le modèle s'il est disponible\n",
    "        if os.path.exists(\"chat_model_tfidf.pth\") and os.path.exists(\"meta_tfidf.pkl\"):\n",
    "            self.assistant.load_all(\"chat_model_tfidf.pth\", \"meta_tfidf.pkl\")\n",
    "            print(\" Model loaded successfully.\")\n",
    "        else:\n",
    "            print(\"Modèle ou métadonnées introuvables. Veuillez d'abord l'entraîner.\")\n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "    # Setup UI components\n",
    "    def setup_ui(self):\n",
    "        # Header\n",
    "        header = Label(self.window, text=\"💬 MedBot Healthcare Assistant\",\n",
    "                       bg=BG_COLOR, fg=\"#00A8E8\", font=(\"Helvetica\", 15, \"bold\"), pady=10)\n",
    "        header.pack(fill=X)\n",
    "\n",
    "        # Chat display (scrollable)\n",
    "        self.chat_display = scrolledtext.ScrolledText(\n",
    "            self.window, wrap=WORD, bg=BOT_COLOR, fg=TEXT_COLOR,\n",
    "            font=FONT, padx=10, pady=10, state=DISABLED\n",
    "        )\n",
    "        self.chat_display.pack(padx=10, pady=10, fill=BOTH, expand=True)\n",
    "\n",
    "        # Bottom frame (input + send button)\n",
    "        bottom_frame = Frame(self.window, bg=BG_COLOR)\n",
    "        bottom_frame.pack(fill=X, side=BOTTOM, pady=5)\n",
    "\n",
    "        self.msg_entry = Entry(bottom_frame, bg=INPUT_COLOR, fg=TEXT_COLOR,\n",
    "                               font=FONT, insertbackground=TEXT_COLOR, relief=FLAT)\n",
    "        self.msg_entry.pack(fill=X, padx=10, pady=10, ipady=8, side=LEFT, expand=True)\n",
    "        self.msg_entry.bind(\"<Return>\", self.send_message)\n",
    "\n",
    "        send_btn = Button(bottom_frame, text=\"Send ➤\", bg=\"#00A8E8\",\n",
    "                          fg=\"white\", font=FONT_BOLD, relief=FLAT,\n",
    "                          command=lambda: self.send_message(None))\n",
    "        send_btn.pack(side=RIGHT, padx=10, pady=5)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Envoi de messages et gestion des réponses\n",
    "    def send_message(self, event):\n",
    "        msg = self.msg_entry.get().strip()\n",
    "        if not msg:\n",
    "            return\n",
    "        self.msg_entry.delete(0, END)\n",
    "\n",
    "        self._insert_message(msg, sender=\"You\", align=\"right\", color=USER_COLOR)\n",
    "\n",
    "        # Simulate typing\n",
    "        self._insert_message(\"typing...\", sender=BOT_NAME, align=\"left\", color=\"#777777\", temp=True)\n",
    "\n",
    "        # Run bot response in a separate thread\n",
    "        threading.Thread(target=self._get_bot_response, args=(msg,)).start()\n",
    "\n",
    "    def _get_bot_response(self, msg):\n",
    "        try:\n",
    "            response = self.assistant.process_message(msg)\n",
    "        except Exception as e:\n",
    "            response = f\" Error: {e}\"\n",
    "\n",
    "        # Remove \"typing...\"\n",
    "        self._delete_last_line()\n",
    "        self._insert_message(response, sender=BOT_NAME, align=\"left\", color=\"#00A8E8\")\n",
    "\n",
    "    \n",
    "    \n",
    "    # Insertion de messages avec des bulles de discussion\n",
    "    def _insert_message(self, text, sender, align=\"left\", color=\"#00A8E8\", temp=False):\n",
    "        self.chat_display.configure(state=NORMAL)\n",
    "\n",
    "        tag_name = f\"{sender}_{align}_{color}\"\n",
    "        self.chat_display.tag_configure(tag_name, justify=align, foreground=color, spacing3=5)\n",
    "\n",
    "        message = f\"{sender}: {text}\\n\\n\"\n",
    "        self.chat_display.insert(END, message, tag_name)\n",
    "        self.chat_display.configure(state=DISABLED)\n",
    "        self.chat_display.see(END)\n",
    "\n",
    "        # If it's temporary (like \"typing...\"), mark position\n",
    "        if temp:\n",
    "            self.last_temp_index = self.chat_display.index(\"end-2c\")\n",
    "\n",
    "    def _delete_last_line(self):\n",
    "        \"\"\"Remove temporary typing message\"\"\"\n",
    "        self.chat_display.configure(state=NORMAL)\n",
    "        self.chat_display.delete(\"end-3l\", \"end-1l\")\n",
    "        self.chat_display.configure(state=DISABLED)\n",
    "\n",
    "    def run(self):\n",
    "        self.window.mainloop()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b1eba58c8b8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Lancer l'application\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mapp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModernChatApp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-8c6790b4604d>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mModernChatApp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MedBot - Healthcare Chat\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBG_COLOR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tk' is not defined"
     ]
    }
   ],
   "source": [
    "# Lancer l'application\n",
    "if __name__ == \"__main__\":\n",
    "    app = ModernChatApp()\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
